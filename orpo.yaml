bind_mounts:
  - container_path: /pfss/mlde/workspaces/mlde_wsp_P_AvemioAG
    host_path: /pfss/mlde/workspaces/mlde_wsp_P_AvemioAG
    propagation: rprivate
    read_only: false
  - container_path: /pfss/mlde/users/sp58jogy
    host_path: /pfss/mlde/users/sp58jogy
    propagation: rprivate
    read_only: false
description: Phi3.5_Mini_ORPO_with_attendee
environment:
  add_capabilities:
    - IPC_LOCK
  drop_capabilities: null
  environment_variables: {}
  force_pull_image: true
  image:
    cuda: determinedai/genai-train:latest
  pod_spec: null
  ports: null
  proxy_ports: null
pbs: {}
resources:
  devices:
    - container_path: /dev/infiniband/
      host_path: /dev/infiniband/
      mode: mrw
  resource_pool: 42_Compute
  slots_per_trial: 96
  weight: 1
slurm: {}
name: Phi3.5_Mini_ORPO_with_attendee
debug: true
searcher:
  name: single
  max_length:
    batches: 1100
  metric: eval_accuracy
  smaller_is_better: false
entrypoint: python -m determined.launch.deepspeed python orpo_finetune.py
max_restarts: 0
hyperparameters:
  model_name: avemio-digital/PHI-3.5-mini-ORPO-after_SFT-16-Epochs
  dataset: avemio-digital/GRAG-ORPO-Long-Context-ShareGPT-Hessian-AI-v2
  dataset_subsets:
    - subset: hard-qa-with-multiple-references
      number_of_samples: 5070
    #- subset: hard-reasoning-de
     # number_of_samples: 3190
    #- subset: hard-reasoning-en
     # number_of_samples: 1970
    #- subset: SauerkrautLM-Fermented-GER-DPO
     # number_of_samples: 3310
    #- subset: SauerkrautLM-Fermented-Irrelevance-GER-DPO
     # number_of_samples: 2000
    #- subset: multi-turn-qa
     # number_of_samples: 3200
    - subset: qa-meeting-topic
      number_of_samples: 900
    - subset: qa-meeting-attendee-topic
      number_of_samples: 900
  training_args:
    output_dir: /pfss/mlde/workspaces/mlde_wsp_P_AvemioAG/soumya_test/output
    max_steps: 1100
    beta: 0.12
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    max_length: 30000
  #  max_prompt_length : 16000
  #  max_completion_length : 16000
    bf16: true
    bf16_full_eval: true
    evaluation_strategy: "steps"
    eval_steps: 400
    logging_strategy: "steps"
    logging_steps: 32
    save_strategy: "steps"
    save_steps: 400
    learning_rate: 5e-7
    gradient_accumulation_steps: 4
    gradient_checkpointing: true
    deepspeed: "ds_configs/ds_config_stage_2.json"
    warmup_ratio: 0.1
    lr_scheduler_type: "cosine"
    optim: "adamw_torch"