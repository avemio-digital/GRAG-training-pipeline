bind_mounts:
  - container_path: /pfss/mlde/workspaces/mlde_wsp_P_AvemioAG
    host_path: /pfss/mlde/workspaces/mlde_wsp_P_AvemioAG
    propagation: rprivate
    read_only: false
  - container_path: /pfss/mlde/users/sp58jogy
    host_path: /pfss/mlde/users/sp58jogy
    propagation: rprivate
    read_only: false
description: mistralnemobase_2407_cpt_stage2
environment:
  add_capabilities:
    - IPC_LOCK
  drop_capabilities: null
  environment_variables: {}
  force_pull_image: false
  image:
    cuda: determinedai/genai-train:latest
  pod_spec: null
  ports: null
  proxy_ports: null
pbs: {}
resources:
  devices:
    - container_path: /dev/infiniband/
      host_path: /dev/infiniband/
      mode: mrw
  resource_pool: 42_Compute
  slots_per_trial: 8
  weight: 1
slurm: {}
name: mistralnemobase_slot8_cpt_stage2
debug: true
searcher:
  name: single
  max_length:
    batches: 12000
  metric: eval_accuracy
  smaller_is_better: false
entrypoint: python -m determined.launch.deepspeed python cpt_finetune.py
max_restarts: 0
hyperparameters:
  model: mistralai/Mistral-Nemo-Base-2407
  dataset: avemio-digital/GRAG-CPT-Hessian-AI
  dataset_subsets:
    - subset: question-answering
      number_of_samples: 231000
    - subset: reasoning-de
      number_of_samples: 231000
    - subset: reasoning-en
      number_of_samples: 231000
    - subset: summarizations
      number_of_samples: 22900
  max_seq_length: 8192
  data_collator:
    on_completions_only: false
    response_template: |
      <|im_start|>assistant
  chat_tokens:
    add_chat_tokens: false
    special_tokens:
      - <|im_start|>
      - <|im_end|>
  training_args:
    output_dir: /pfss/mlde/workspaces/mlde_wsp_P_AvemioAG/soumya_test/output
    max_steps: 12000
    per_device_train_batch_size: 1  # Reduced batch size
    per_device_eval_batch_size: 1
    bf16: true  # Enable mixed precision
    evaluation_strategy: steps
    eval_steps: 700
    logging_strategy: steps
    logging_steps: 10
    save_strategy: epoch
    save_steps: 1
    learning_rate: 0.00001
    gradient_accumulation_steps: 16  # Increased gradient accumulation steps
    deepspeed: ds_configs/ds_config_stage_2.json
    warmup_ratio: 0.1
    seed: 43
    gradient_checkpointing: true
